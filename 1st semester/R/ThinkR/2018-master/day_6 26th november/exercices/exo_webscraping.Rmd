---
title: "webscraping"
output: html_document
---

```{r include = FALSE}





```

### Scraping

Get the name of all members from `https://thinkr.fr/expert-logiciel-r/`

```{r}




```

Get the list of the last posts from `https://thinkr.fr/le-blog/`

```{r}





```


Extract the "milestones" table from `https://en.wikipedia.org/wiki/R_(programming_language)`

```{r}





```

Extract all the titles from `https://cran.r-project.org/doc/manuals/r-release/R-intro.html`

```{r}




```

Do the same, but put the output in a dataframe with three columns : type, class, and text.

```{r}











```

Put the previous code in a function

```{r}












```

Create a function that takes an url, a header level, and return all the headers from this page in a data.frame. 

```{r}












```

Create a function that tried to connect to a url, and return `NULL` if it fails to read the html. 

```{r}

```

Create a function takes a list of urls and a node. The function will map over the list of urls, and return the nodes text of each page. The output should be a named list, and the unavailable urls should not be included. 

```{r}







```

Try this function on : 

`c("https://thinkr.fr", "https://cran.r-project.org", "https://rtask.thinkr.fr/", "https://abcdr.thinkr.fr/", "not_working.com")`

```{r}

```

Create a function `random_walk()` that : 

+ Takes an url
+ Verify if the link is scrapable (i.e authorized for robots), if not, return the url
+ Get all the links from the page
+ Select a random link from the result
+ Start again, until the selected link is not authorized for robots.

```{r}













```

Try on `random_walk("https://thinkr.fr")`
