---
title: "Webscraping"
output: formation::exo_rmd
params:
  hide:
    value: TRUE
  warning:
    value: FALSE
  message:
    value: FALSE
  results:
    value: 'hide'
  path: 
    value: "image/"
---

```{r include = FALSE}
knitr::opts_chunk$set(message = params$message, warning = params$warning, results = params$results)
library(knitr)
if (params$hide) {
  formation::hide_code()
}
```

### Scraping

Get the name of all members from `https://thinkr.fr/expert-logiciel-r/`

```{r}
library(rvest)
'https://thinkr.fr/expert-logiciel-r/' %>% 
  read_html() %>% 
  html_nodes(".team-author-name") %>% 
  html_text()
```

Get the list of the last posts from `https://thinkr.fr/le-blog/`

```{r}
'https://thinkr.fr/le-blog/' %>% 
  read_html() %>% 
  html_nodes("h2 a") %>% 
  html_text()
```


Extract the "milestones" table from `https://en.wikipedia.org/wiki/R_(programming_language)`

```{r}
a <- "https://en.wikipedia.org/wiki/R_(programming_language)"  %>% 
  read_html() %>% 
  html_table(fill = TRUE) %>%
  .[[2]]
  
```

Extract all the titles from `https://cran.r-project.org/doc/manuals/r-release/R-intro.html`

```{r}
"https://cran.r-project.org/doc/manuals/r-release/R-intro.html"  %>% 
  read_html() %>% 
  html_nodes("h1, h2, h3, h4, h5, h6") %>% 
  html_text()
```

Do the same, but put the output in a dataframe with three columns : type, class, and text.

```{r}
library(purrr)
"https://cran.r-project.org/doc/manuals/r-release/R-intro.html"  %>% 
  read_html() %>% 
  html_nodes("h1, h2, h3, h4, h5, h6") %>% 
  map_df(function(x){
    tibble(
      type = html_name(x),
      class = html_attr(x, "class"), 
      content = html_text(x)
    )
  })
```

Put the previous code in a function

```{r}
extract_headers <- function(url){
  url  %>% 
  read_html() %>% 
  html_nodes("h1, h2, h3, h4, h5, h6") %>% 
  map_df(function(x){
    data.frame(
      type = html_name(x),
      class = html_attr(x, "class"), 
      content = html_text(x)
    )
  })
}
```

Create a function that takes an url, a header level, and return all the headers from this page in a data.frame. 

```{r}
extract_headers_bis <- function(url, header){
  url  %>% 
  read_html() %>% 
  html_nodes(header) %>% 
  map_df(function(x){
    tibble(
      type = html_name(x),
      class = html_attr(x, "class"), 
      content = html_text(x)
    )
  })
}
extract_headers_bis("https://thinkr.fr", "h2")
```

Create a function that tried to connect to a url, and return `NULL` if it fails to read the html. 

```{r}
possible_read_html <- possibly(read_html, otherwise = NULL)
```

Create a function takes a list of urls and a node. The function will map over the list of urls, and return the nodes text of each page. The output should be a named list, and the unavailable urls should not be included. 

```{r}
extract_node <- function(l, node){
  map(l, possible_read_html) %>%
    set_names(l) %>%
    compact() %>%
    map(~ html_nodes(.x, node))  %>%
    map(~ html_text(.x))
}
```

Try this function on : 

`c("https://thinkr.fr", "https://cran.r-project.org", "https://rtask.thinkr.fr/", "https://abcdr.thinkr.fr/", "not_working.com")`

```{r}
extract_node(c("https://thinkr.fr", "https://cran.r-project.org", "https://rtask.thinkr.fr/", "https://abcdr.thinkr.fr/", "not_working.com"), "h1")
```

Create a function `random_walk()` that : 

+ Takes an url
+ Verify if the link is scrapable (i.e authorized for robots), if not, return the url
+ Get all the links from the page
+ Select a random link from the result
+ Start again, until the selected link is not authorized for robots.

```{r}
library(robotstxt)
random_walk <- function(url){
  while( robotstxt::paths_allowed(url) ){
    url <- url %>% 
      read_html() %>%
      html_nodes("a") %>%
      html_attr("href") %>%
      sample(1) 
    message("moving to:")
    message(url)
  }
  return(url)
}
```

Try on `random_walk("https://thinkr.fr")`
