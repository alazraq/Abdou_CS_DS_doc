---
title: "PCA"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Principal components analysis

## Lecture

Before starting, you should watch the lecture by François Husson on a practical presentation of PCA. [french](https://www.youtube.com/playlist?list=PLnZgp6epRBbRn3FeMdaQgVsFh9Kl0fjqX),[english](https://www.youtube.com/playlist?list=PLnZgp6epRBbQuZSHJ5hBzxdDZNzBFKBoB)

Principal component analysis (PCA) summarizes a data table where the observations are described by (continuous) quantitative variables. PCA is used
to study the similarities between individuals from the point of view of all of the variables and identifies individuals profiles. It
is also used to study the linear relationships between variables (based on correlation coefficients). The two studies are linked, which means that the individuals or groups can be characterized by the variables and the links between variables illustrated
from typical individuals. 
More details are in the lecture slides.

To perform PCA, we use the PCA function from the FactoMineR package. A [website](http://factominer.free.fr) is dedicated to this package. 
The code associated to the lecture slides is the following:

```{r eval = FALSE}
library(FactoMineR)
#Expert <- read.table("http://factominer.free.fr/course/doc/data_PCA_ExpertWine.csv",
#header = TRUE, sep = ";", row.names = 1)
Expert <- read.table("data/Expert_wine.csv", header = TRUE, sep = ";", row.names = 1)
res.pca <- PCA(Expert, scale = T, quanti.sup = 29:30, quali.sup = 1)
summary(res.pca)
barplot(res.pca$eig[,1], main = "Eigenvalues", names.arg = 1:nrow(res.pca$eig))
plot.PCA(res.pca, habillage = 1)
res.pca$ind$coord; res.pca$ind$cos2; res.pca$ind$contrib
plot.PCA(res.pca, axes = c(3, 4), habillage = 1)
dimdesc(res.pca)
plotellipses(res.pca, 1)
write.infile(res.pca, file = "my_FactoMineR_results.csv")
```

A Shiny interface has been developed for FactoMineR, you should install the Factoshiny package, it may give you an idea of what interfaces you should produce to facilitate the use of the methods you develop.

```{r eval = FALSE}
library(Factoshiny)
data(decathlon)
PCAshiny(decathlon)
```

A very useful package is  FactoInvestigate. It generates an automatic report and interpretation of FactoMineR principal component analyses. The main function provided by the package is the function Investigate(), which can be used to create either a Word, PDF or a HTML report.

```{r eval = FALSE}
library("FactoInvestigate")
# Principal component analyses
Investigate(res.pca, file = "PCA.Rmd", document = "pdf_document", 
            parallel = TRUE)
```

The aim of this package is also to encourage you going further in the interpretation!

Finally, we should mention that there exists a package that has embedded FactoMineR within ggplot2!

```{r eval = FALSE}
library(factoextra)
```



References on principal component methods and on the FactoMineR package:
Exploratory Multivariate Analysis by Example
using R, Husson, Lê, Pagès (2017), Chapman & Hall
Multiple Factor Analysis by Example using R, Pagès (2015), CRC Press.

## Detailed derivation of Principal Component Analysis

Let $(X_i)_{1\le i \le I}$ be the $I$ observations in $\mathbb{R}^K$ and $X$ the $\mathbb{R}^{I\times K}$ matrix such that the $i$-th line of $X$ is $X_i$.  Principal Component Analysis aims at finding a subspace of $\mathbb{R}^K$ such that 
the observations $(X_{i})_{1\le i \le I}$ are simultaneously close to their projections in this subspace or equivalently a subspace that maximizes the variance of the projected points.
In the following, we assume that the data are centered i.e. $I^{-1}\sum_{i=1}^I X_{i} = (0, ...,0)$.

### (1) Maximizing the variance (of the projected points)
The first direction $u_1\in \mathbb{R}^K$ is the vector which  maximizes the empirical variance of the one dimensional vector $(a’X_1,\ldots,a’X_I)$ for $a\in\mathbb{R}^K$ such that $\|a\|^2=a'a=1$:
$$
u_1 \in \underset{\|a\|=1}{\mathrm{Argmax}}\left\{\, I^{-1}\sum_{i=1}^I \langle X_{i},a \rangle^2\,\right\}.
$$
Then, for all $k\ge 2$, $u_k$ is the vector which  maximizes the empirical variance of the one dimensional vector $(a’X_1,\ldots,a’X_I)$ on the set $\{a\in\mathbb{R}^K\,;\, \|a\| =1\,,\, a\perp u_1,\ldots,u_{k-1}\}$:
$$
u_k \in \underset{\|a\|=1\,,\, a\perp u_1,\ldots,u_{k-1}}{\mathrm{Argmax}} \left\{I^{-1}\sum_{i=1}^I \langle X_{i},a \rangle^2\,\right\}.
$$


### (2) Minimizing the reconstruction error (the distances between individuals and their projection)

For any dimension $Q\le K$, PCA computes the linear span $u$ defined by:
$$
u \in \underset{\mathrm{dim}(u)\le Q}{\mathrm{Argmin}} \left\{\sum_{i=1}^I \left\|X_{i} - \pi_u(X_{i})\right\|^2\right\}\,,
$$
where $\pi_u$ is the orthogonal projection onto $u$. 

### Relationship with SVD

PCA relies on the singular value decomposition of the matrix $X$. Let $r$ be the rank of the matrix $X$ and $\lambda_1 \ge \ldots\ge \lambda_r>0$ be the nonzero eigenvalues of the empirical variance:
$$
S= I^{-1}X’X = I^{-1}\sum_{i=1}^I X_i(X_i)'\,.
$$
There exist two orthonormal families $(v_j)_{1\le j \le r}$ in $\mathbb{R}^I$ and $(u_j)_{1\le j \le r}$ in $\mathbb{R}^K$ such that:
$$
I^{-1}XX’v_j = \lambda_jv_j\quad\mbox{and}\quad I^{-1}X’Xu_j = \lambda_ju_j\,.
$$
$(\lambda_j)_{1\le j \le r}$ are referred to as the eigenvalues values and their squared root as the singular values, $(v_j)_{1\le j \le r}$ as the left-singular vectors and $(u_j)_{1\le j \le r}$ the right-singular vectors. Note that $S= u\Lambda u'$ with its eigenvalue decomposition can also be written with $S  = \sum_{j=1}^r \lambda_j u_j u_j'$



For all $Q\le r$, let $H_Q$ be the linear space spanned by $\{u_1,\ldots,u_Q\}$. We first show that $H_Q$ is solution to the problem of maximizing the variance of the projected points (1). Let $a\in\mathbb{R}^K$ with $\|a\|^2 =a'a=1$. Then,
$$
Var(X_i'a) = I^{-1}\sum_{i=1}^I \langle X_{i},a \rangle^2 = I^{-1}\sum_{i=1}^I  a'X_{i}(X_i)'a = a'\left(I^{-1}X'X\right)a = \sum_{j=1}^r \lambda_j(a'u_j)(u_j'a)\,.
$$
Note that, since $\lambda_1$ is the largest eigenvalues, and  $u_j$ form an orthonormal basis and $\sum_{j=1}^r (a'u_j)(u_j'a) = \|a\|^2=1$ we have:
$$
\sum_{j=1}^r \lambda_j(a'u_j)(u_j'a)\le \lambda_1\sum_{j=1}^r (a'u_j)(u_j'a)\le \lambda_1\,.
$$

On the other hand, if we choose $a=u_1$, as $u_1'u_j = 0$ for $u\le j \le r$ and $\|u_1\|^2=u_1'u_1=1$:
$$
\sum_{j=1}^r \lambda_j(a'u_j)(u_j'a) = \lambda_1\,.
$$
This yields:
$$
u_1 \in \underset{\|a\|=1}{\mathrm{Argmax}} \sum_{i=1}^I \langle X_{i},a \rangle^2\,.
$$
and $Var(X_i'u)= \lambda_1$. 
Then, for $k\ge 2$, let  $a\in\mathbb{R}^K$ such that $a\perp u_1,\ldots,u_{k-1}$. 
$$
\sum_{j=1}^r \lambda_j(a'u_j)(u_j'a)\le \lambda_k\sum_{j=k}^r (a'u_j)(u_j'a)\le \lambda_k\,.
$$
If we choose $a=u_k$, as $a'u_j = 0$ for $1\le j \le k-1$ $\|a\|^2=a'a=1$ and:
$$
\sum_{j=1}^r \lambda_j(a'u_j)(u_j'a) = \lambda_k\,,
$$
which proves that:
$$
u_k \in \underset{\|a\|=1\,,\, a\perp u_1,\ldots,u_{k-1}}{\mathrm{Argmax}} \sum_{i=1}^I \langle X_{i},a \rangle^2\,.
$$

The space $H_Q$ is also solution to the problem of minimization of the reconstruction error (2). As for all $1 \le i \le I$, $\pi_{H_Q}(X_i) = \sum_{j=1}^Q (X’_iu_j)u_j$, for all linear span $u$ in $\mathbb{R}^K$ such that $\mathrm{dim}(u)\le Q$,
$$
\sum_{i=1}^I \left\|X_i - \pi_{H_Q}(X_i)\right\|^2 = \sum_{j=Q+1}^r\lambda_j\le \sum_{i=1}^I \left\|X_i - \pi_{u}(X_i)\right\|^2\,,
$$
which proves that: 
$$
H_Q \in \underset{\mathrm{dim}(u)\le Q}{\mathrm{Argmin}} \left\{\sum_{i=1}^I \left\|X_{i} - \pi_u(X_{i})\right\|^2\right\}\,,
$$

The vectors $(F_j = X u_j)_{1\le j \le Q}$ are the principal components. Note that for all $1 \le i \le I$,
$$
\pi_{H_Q}(X_i) = \sum_{j=1}^Q (F_j)_iu_j\,.
$$ 
The vectors $(F_j = X u_j)_{1\le j \le Q}$ are orthogonal eigenvectors of $I^{-1}XX’$.

The reconstruction error can be written as $$ \sum_{i=1}^{I}\| X_{i}- uu'X_{i}\|^2 = \|X - Fu'\|^2 $$

When keeping $Q$ dimensions, the matrix $\hat X_{I \times Q} = F_{I \times Q}u_{Q \times K}'$ is know as the reconstructed (fitted) matrix by PCA. It corresponds to the best approximation in $Q$ dimensions of the data matrix $X$.

### Explained variance and contribution, quality of projection

- The percentage of variance explained by the first $Q$ dimensions is:
$$
\alpha_Q = \frac{(1/I) \sum_{i=1}^I\|\pi_{H_Q}(X_{i})\|^2}{ (1/I)\sum_{i=1}^I\|X_{i}\|^2} = \frac{(1/I) \sum_{i=1}^I\|\pi_{H_Q}(X_{i})\|^2}{ trace(S)} =  \frac{\sum_{i=1}^Q\lambda_i}{\sum_{i=1}^K\lambda_i}\,.
$$
The sum of the variance of all the variables is equal to the number of variables when the variables are scaled. $trace(S)=K$.


- Quality of the projection - cos2

Let's consider the coordinate of a variable $X_{.k}$ (the $k$-th column of $X$) and $D$ the diagonal matrix with $1/I$ on its diagonal.  $\langle a, b\rangle_D = a'Db$.
$$
\rho_k(j) = \frac{\langle X_{.k}, v_j\rangle_D}{\|X_{.k}\|_D\|v_j\|_{D}}\,,
$$


Let $W_Q = \mathrm{span}\{v_1,\ldots,v_Q\}$. Note that for all $1 \le i \le K$,
$$
\|\rho_k\|_D^2 = \frac{\|\pi_{W_Q}(X_{.k})\|_D^2}{\|X_{.k}\|_D^2}\le 1\,.
$$
$\|\rho_k\|$ measures the quality of the projection of $X_{.k}$ on the first $Q$ principal components. When the variables are scaled, $\|X_{.k}\|_D^2=1$ the cos^2 for a variable corresponds to its coordinate squared: $(\langle X_{.k}, v_j\rangle_D)^2$.

The quality of projection can also be defined for the individuals. 