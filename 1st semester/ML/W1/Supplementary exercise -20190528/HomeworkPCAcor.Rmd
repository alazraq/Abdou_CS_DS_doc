---
title: 'Homework: Reconstruction with PCA'
output:
  html_document: default
  pdf_document: default
---
## PCA by hand 

```{r libraries}
library(FactoMineR)
library(ggplot2)
```

__(Q1)__ You have a data set with $I$ rows and $K$ variables. Your aim is to find two synthetics variables that "best summarize" the individuals. Explain what is your suggestion and gives the object (the output in the PCA function) where you can retrieve the  result.

Let us call the data matrix $X\in \mathbb{R}^{I\times K}$. We perform PCA on $X$. The first two principal components are also known as the scores or the coordinates of the observations on the dimensions. This corresponds to the matrix $F$ in the slides of size $I \times Q$, with $Q$ the dimension of the subspace. The scores are eigenvectors of the inner-product matrix $WD=XX'D$ with $D$ the diagonal matrix with $1/I$ on the diagonal and their variance is equal to the eigenvalue. 
The code goes as follows
```{r eval =FALSE}
# X is a data matrix

library(FactoMineR)
data(decathlon)
X <- decathlon[, 1:10]
res_pca <- PCA(X, graph = F)
ind_summary <- res_pca$ind$coord[,1:2]
head(ind_summary)
```
If one want to apply another statistical method on the results of PCA, he would apply the method on this matrix $F_{n \times Q}$.


Import the data Expert_wine.csv. The aim is to do a PCA on this data set, without using the function PCA of FactoMineR. Be careful not to include qualitative variables. First, run the function PCA of FactoMineR. 
Note that by default of the PCA function, the data is centered and standardized by columns. Besides, for each column $k$, the empirical variance used to standardize the column is 

$$ \hat{\sigma}_k = \frac{1}{I}\sum_{i=1}^I (x_{ik}-\bar{x}_k) \: , \qquad  
\bar{x}_k = \frac{1}{I} \sum_{i=1}^I x_{ik}
$$

where $I$ is the number of rows of the data (here $I=10$). Be careful, in R the **scale** function computes the empirical variance as follows:

$$ \hat{\sigma}_k = \frac{1}{I-1}\sum_{i=1}^I (x_{ik}-\bar{x}_k) \: , \qquad  
\bar{x}_k = \frac{1}{I} \sum_{i=1}^I x_{ik}
$$

```{r eval =FALSE}
wine <- read.csv("../data/Expert_wine.csv", sep=";", row.names = 1)
```

__(R1)__ In a few lines of code, recover by yourself the eigenvalues found with PCA of FactoMineR.

__(R2)__ In a few lines of code, recover the projections of the individuals (the coordinates of the individuals) on the 2 first principal axis.

```{r eval =FALSE}

Expert <- read.table("../data/Expert_wine.csv", header=TRUE, sep=";", row.names=1)

library(FactoMineR)
resPCA<-PCA(Expert, quali.sup = 1)

M=as.matrix(Expert[,-1])
n<-nrow(M)
H<-diag(rep(1,n))-(1/n)*rep(1,n)%*%t(rep(1,n))
S<-(1/n)*t(M)%*%H%*%M
InvTheta<-diag(1/sqrt(diag(S)))
R<-InvTheta%*%S%*%InvTheta

REig<-eigen(R, symmetric = T)

aa=sqrt(10/9)*scale(M)%*%REig$vectors
aa[,1]-resPCA$ind$coord[,1]

svdM<-svd(sqrt(10/9)*scale(M))
svdM$d[1:9]-resPCA$eig[,1]

svdM<-svd(sqrt(1/10)*sqrt(10/9)*scale(M))
svdM$d[1:9]^2-resPCA$eig[,1]

```

__(Q2)__ From the preceding question, you obtain 2 vectors of dimension $10$. Show (theoretically) that their empirical variances are respectively the first and the second eigenvalues of the PCA (given by PCA of FactoMineR for example).

Let us show that $var(F_{.1})=\lambda_{1}$ :
$$ var(F_{.1})=\frac{1}{I}\sum_{i=1}^{I}F_{i1}^2-(\frac{1}{I}\sum_{i=1}^{I}F_{i1})^2
$$
We know that $u_1$ is the first eigenvector of the covariance matrix $\frac{X'X}{I}u_1= \lambda_1 u_1$.  You can premultiply each side by $u_1'$, to get  $u_1' \frac{X'X}{I}u_1= \lambda_1 u_1' u_1$. Since $\|u_1\|^2=1$, we have $u_1' \frac{X'X}{I}u_1= \lambda_1$.

$F_{.1}$ is defined as the coordinates of the observations on $u_1$, which correspond to the projections of the observations on $u_1$, i.e. the inner product of the observation and $u_1$: $F_{.1}=Xu_{.1}$. Consequently, $\frac{1}{I}\sum_{i=1}^{I}F_{i1}^2 = \frac{1}{I}F_{.1}'F_{.1}=\frac{1}{I} u_1' {X'X}u_1 =  = \lambda_1$.
hence one needs to show that $\frac{1}{I}\sum_{i=1}^{I}F_{i1}=0$ Now remembering that the data is centered :
$$\frac{1}{I}\sum_{i=1}^{I}F_{i1}=\frac{1}{I}\sum_{i=1}^{I}\sum_{k=1}^{K}x_{ik}u_{k1}=\sum_{k=1}^{K}u_{k1}\bar{x}_{k}=0 
$$

Remark : Why do we have $\sum_{k=1}^{K}\lambda_k=K$ By definiton : 
$$Tr(S)=Tr(\frac{X^TX}{I})=\sum_{k=1}^{K}\lambda_k
$$
And the matrix is standardized : 
$$Tr(S) = \frac{1}{I}\sum_{i=1}^{I}\sum_{k=1}^{K}x_{ik}^2= \sum_{k=1}^{k}1=K
$$

## Approximation of the ozone data

Air pollution is currently one of the most serious public health worries worldwide.
Many epidemiological studies have proved the influence that some chemical
compounds, such as sulphur dioxide (SO2), nitrogen dioxide (NO2), ozone
(O3) can have on our health.
Associations set up to monitor air quality are active all over the world to
measure the concentration of these pollutants. They also keep a record of
meteorological conditions such as temperature, cloud cover, wind, etc.
Here the final aim is to analyse the relationship between the maximum daily ozone
level  and meteorological variables. We have at our disposal 112 observations
collected during the summer of 2001 in Rennes. Before trying to build a prediction model, we will inspect the data with a PCA and illustrate the reconstruction property of PCA.
The variables available are
maxO3 (maximum daily ozone), maxO3v (maximum daily ozone the previous day), T12 (temperature at midday), T9, T15 (Temp at 3pm), wind (direction),
rain and Vx12 (projection of the wind speed vector on the east-west
axis at midday), Vx9 and Vx15, as well as the Nebulosity (cloud) Ne9, Ne12, Ne15. The data is contained in the file ozone.txt.

* Perform the PCA on the ozone data (also using the categorical variables) and give a short interpretation of the first dimensions of variability (interpretation of the observations plot, correlation between variables, percentage of variability, etc.). The variable maxO3 that we want to predict using a regression model for instance can be put as a supplementary variable in the analysis. Explain the interest of perfoming this PCA before a regression.

```{r, eval = TRUE, include = TRUE}
library(FactoMineR)
ozone <- read.table("data/ozone.txt", header = T, sep = " ", row.names = 1)
res.pca <- PCA(ozone, quanti.sup = 1, quali.sup = c(12, 13))
```

The interpretation is carried out in many steps:
1) Variables:
1.1) Correlation between variables:  

All variable vectors being quite near from the boundary of the correlation circle on the variables plot, the variables are well projected on the 2 dimensional subspace.
Hence, we can have an idea of which linear dependency exists between the different initial variables. This plot provides a visualization of the correlation matrix.

3 groups of variables stand out:
one group constituted with the 4 variables ```T9```, ```T12```, ```T15``` and ```maxO3y```, the second with the 3 variables ```Ne9```, ```Ne12``` and ```Ne15``` and the last one with the 3 variables ```Vx9```, ```Vx12``` and ```Vx15```. Within each of this group, variables are strongly positively correlated.

We also note that the variables ```Vx9```, ```Vx12``` and ```Vx15``` are negatively correlated to the variables ```Ne9```, ```Ne12``` and ```Ne15```.

1.2) Correlation between variables and principal components:

The vectors corresponding to variables ```Ne9```, ```Ne12``` and ```Ne15``` are pointing in the opposite direction from the first principal component which means that those 3 variables are  very strongly negatively correlated to the first principal component.

Then
2) Observations study.
3) Relationship between the two studies, observations and variables.
4) Comment the percentage of variability
5) Look at the supplementary variables to help for the interpretation

It is very usual to put the variable we want to predict in a regression model as supplementary variable. It allows to understand
the relationship between the X variables so that we can understand which variable would be selected when using selection methods in a linear regression to explain ```maxO3``` from the other variables.
It allows as well to study the relationship between Y and the main dimensions of variability of X

6) Analyse if needed other dimensions.

The minimum that we expect in term of interpretation can be obtained automatically by using the R package FactoInvestigate. Then, the aim is to go further.
Look at 
```{r, eval = FALSE}
library(FactoInvestigate)
Investigate(res.pca)
```


* With the PCA outputs, one could reconstruct the data.  First, reconstruct the data with one dimension. You should be aware that with the results of PCA, you could reconstruct the centered and scaled data, so that you may need to add the means and multiply by the standard deviations to get the approximation of the original matrix.

We remind that we can get a $1$-dimensionnal approximation of the true data by the following procedure: $\hat{X^1}=F_{.1}u_1'= Xu_1u_1'$ 
 $F$ is the matrice of size $I \times 1$  containing the first principal components, also known as the scores or the coordinates of the $I$ observations on the principal component. It is given in ```res.pca$ind$coord)[,1,drop=FALSE]``` of the PCA function output.  For $u$, we have that $u_1 =\frac{G_{.1}}{\sqrt{\lambda_1}}$ with $G$ the coordinates of the variables on the first dimension given in the output ```(res.pca$var$coord)[,1,drop=FALSE]``` of the PCA function.

```{r, eval =TRUE, include = TRUE}
# Reconstruction with one dimension
HatX1dim <- (1/sqrt(res.pca$eig[1, 1]))*(res.pca$ind$coord[, 1, drop = FALSE]%*%
 t(res.pca$var$coord[, 1, drop = FALSE]))
 dim(HatX1dim)
ozmean <- apply(ozone[, 2:11], 2, mean)
# the PCA function uses "the population" covariance matrix, i.e. 1/n(X'X) whereas the function sd uses the unbiased estimator of a variance for a sample 
ozsd <- apply(ozone[, 2:11], 2, sd) * sqrt((nrow(ozone)-1)/nrow(ozone))
HatX1dim  <- sweep(HatX1dim, 2, ozsd, FUN = "*")
HatX1dim  <- sweep(HatX1dim, 2,  ozmean, FUN = "+")
```

* Calling $\hat{X}$ the reconstructed matrix, observe the difference between $X$ and $\hat X$ by plotting the variable maxO3y for the two matrices in function of the observations. Comment.

```{r eval = TRUE, include=TRUE}
library(ggplot2)
ggplot() + 
geom_line(data=ozone, aes(x=seq(1, length(ozone$maxO3v)), y=maxO3v, colour="ozone")) + 
geom_line(data=data.frame(HatX1dim), aes(x=seq(1, length(data.frame(HatX1dim)$maxO3v)), y=maxO3v, colour="HatX1dim")) +scale_colour_manual(name="Legend",
    values=c(ozone="green", HatX1dim="red"))+ggtitle("Reconstruction of maxO3v") +
  labs(x=" ",y="maxO3v") 
```

The reconstruction of the data with only one dimension manages to catch the general pattern of variability of the variable ```maxO3y``` but fails to detect the biggest deviations from mean. Note that ```maxO3y``` is not well projected on the correlation circle and we could not expect to a good reconstruction with one dimension.

<!--* You can observe the difference between $X$ and $\hat X$ (noted  HatX1dim) by looking at the following plot for the variable maxO3y:

```{r eval = FALSE}
 plot(ozone[,"maxO3y"], type = "l")
 lines(HatX1dim[,"maxO3v"], col = 2)
```
-->


* Build a function to reconstruct the data with a specific number of dimensions. Then, reconstruct the data with 2 to 4 dimensions and represent different variables (observed and estimated). Comment your results with respect to the correlation circle.

With $Q$ dimensions, we can do the same thing $\hat X= Fu'$. Note that it is related to the SVD of $X$ as follows:
If $U\Lambda^{\frac{1}{2}}V'$ is the singular value decomposition of the centered (and scaled data) $X$ then $F=U\Lambda^{\frac{1}{2}}$  and $u=V$.

```{r,eval=TRUE,include = TRUE}
reconst_data <- function(res.pca, ncp = ncp)
{
  quali.sup <- eval(res.pca$call$call$quali.sup)
  quanti.sup <- eval(res.pca$call$call$quanti.sup)
  var_sel <- c(1:dim(res.pca$call$X)[2])[-c(quanti.sup,quali.sup)]
  don <- res.pca$call$X
  HatXddim <- as.matrix(res.pca$ind$coord)[,1:ncp, drop=FALSE] %*%
    t(sweep(as.matrix(res.pca$var$coord)[,1:ncp,drop=FALSE],2,sqrt(res.pca$eig[1:ncp,1]),FUN="/"))
  donmean <- apply(don[, var_sel], 2, mean)
  donsd <- apply(don[, var_sel], 2, sd) * sqrt((nrow(don)-1)/nrow(don))
  HatXddim  <- sweep(HatXddim, 2, donsd, FUN = "*")
  HatXddim  <- sweep(HatXddim, 2,  donmean, FUN = "+")
  return(HatXddim)
}
```


You should check that  ```reconst_data``` and the function ```reconst``` of the FactoMineR package outputs the same results.

We should reconstruct the data with more dimensions.
The rule is the following one: if a variable is highly correlated to the 1st dimension, 1 dimension will be enough to have a very accurate reconstruction (like Ne9), if it is related to both the first and second dimension, you need at least two dimensions as illustrated with Vx12: 

```{r eval = TRUE, include=TRUE}

plot1=ggplot() + 
geom_line(data=ozone, aes(x=seq(1, length(ozone$Vx12)), y=Vx12, colour="ozone")) + 
geom_line(data=data.frame(HatX1dim), aes(x=seq(1, length(data.frame(HatX1dim)$Vx12)), y=Vx12, colour="HatX1dim")) +scale_colour_manual(name="Legend",
    values=c(ozone="green", HatX1dim="red"))+ggtitle("Reconstruction of Vx12") +
  labs(x=" ",y="Vx12") 

 # The function reconst in FactoMineR implements the recontruction of the data by PCA
 recon <-reconst(res.pca, 2)
plot1+ geom_line(data=data.frame(recon), aes(x=seq(1, length(data.frame(recon)$Vx12)), y=Vx12, colour="HatX2dim")) +scale_colour_manual(name="Legend",
    values=c(ozone="green", HatX1dim="red", HatX2dim="purple"))+ggtitle("Reconstruction of Vx12") +
  labs(x=" ",y="Vx12") 

```
The fitted values  are closer to the data. Note that if one uses all the dimensions to reconstruct, you will end up with the matrix $X$.

## Reconstruction of an image

Import the data missing.jpg or breton.png  and reconstruct the image with 1 to 100 dimensions. There are different packages dedicated to images. 
With the following lines of code, you could  read an image, reprensent it and then transform it in a grey scale. (Be careful that you may need to execute the code in the R console and not by knitting).

```{r eval=FALSE}
source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
```

```{r eval = FALSE,echo=TRUE}
library("EBImage")
img <- readImage("data/missing.jpg")
display(img, method = "raster")

library(ripa)
r <- imagematrix(img, type = "grey")
display(r, method = "raster")
```

Using the grey image (r), reconstruct the image with 1 to 100 dimensions and export the reconstructed images in a folder with the names "missing_ncp_1" to "missing_ncp_100". You could use the packages "jpeg" and "png" to export the image with functions 'writePNG' for instance.  Comment.

```{r include = TRUE, eval = FALSE}
library(FactoMineR)
#library(jpeg)
library(png)
for (ncp in c(1:100)){
  res.pca <- PCA(r, ncp, graph = F)
  writePNG(reconst(res.pca,ncp=ncp), paste("missing_ncp",ncp,".png",sep=""))
}
```

Even with 18 dimensions, most of the layout is recovered and with 57 dimensions, we are perfectly able to read which alphabetic characters are written.

<!--

## Reconstruct an image

Import the data missing.jpg or breton.png  and reconstruct the image with 1 to 100 dimensions. Export the outputs. Comment. 

There are different packages dedicated to images, we can use the following:

```{r eval = TRUE}
#source("https://bioconductor.org/biocLite.R")
#biocLite("EBImage")
library("EBImage")
img <- readImage("data/missing.jpg")
display(img, method = "raster")
```

```{r eval = FALSE}
library(ripa)
r <- imagematrix(img, type = "grey")
display(r, method = "raster")
# Then, you can reconstruct this object r with 1 to 100 dimensions and visualize the results.
```
You can also import the images with readPNG or readJPEG from the png and jpeg packages.

```{r include = FALSE, eval = FALSE}
Revoir images.... http://mpmendespt.github.io/
library(FactoMineR)
for (ncp in c(20, 50, 100)){
  res.pca<-PCA(r,ncp=ncp,graph=F)
image.recons<-writePNG(reconst(res.pca,ncp=ncp),paste("missing_ncp",ncp,".png",sep=""))

l1g <- imagematrix(l1, type = "grey")
#plot(l1g, useRaster = TRUE)
display(l1g, method = "raster", all=TRUE)

You could use the function readPNG and writePNG.

image<-readJPEG("missing.jpg")
image.recon <- writeJPEG(....., "/missing.recon")
 library(ripa)
install.packages("jpeg")
library(jpeg)
install.packages("png")
library(png)
image <- readPNG("data/breton.png")
library("EBImage")
display(image, method = "raster")
image.recon <- writePNG(....., "breton.recon")

}

```
-->